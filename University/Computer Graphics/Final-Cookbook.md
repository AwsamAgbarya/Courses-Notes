# [[Color]] 
* #### Light Spectrum
	- Light wave's wavelength/frequency determines what color it represents
	- Monochromatic colors are colors that can be produced via frequency.
	- white is an achromatic color because its a mixture of various wavelengths
	- Visible light spectrum 380-700nm:
		1. Violet 380-450
		2. Blue 450-495
		3. Green 495-570
		4. Yellow 570-590
		5. Orange 590-620
		7. Red 620-750
	- Order of wavelength (small to big): Gamma rays -> X rays -> UV rays -> visible rays -> Infrared rays -> Microwaves -> Radiowaves -> Long radio waves
	* Geometric optics: light behaves like rays shot in a straight line, can do reflection and refraction
	* Quantum optics: light are individual light particles with energy
	* Wave optics: light is a wave emanating from a point in all directions
	* Radiometry: Physical description of electromagnetic energy
	* Photometry: measurement of energy induced by electromagentic waves in the visible light spectrum
* #### Eye:
	* Rods and cones are photoreceptors in the retina
	* Center of Fovea contains a lot of cones which are responsible for detecting the colors in high light (LMS).
		* 48% Green 534
		* 42% Red 564
		* 10% Blue 420
	* if the maxima of green and red overlap, you will not be able to differentiate between red and green
	* outside the fovea along the retina contains a lot of rods which are responsible for recording the light that hits our eyes (can only go up to green 500)
		* 10^4 Daylight
		* 10^2 Monitor
		* 1 Reading
		* 10^-2 lowest color
		* 10^-6 lowest threshold for vision
	* Skoptic is night vision
	* photopic is daylight vision
* #### Color
	* If cones respond in a similar manner to two colors then they're the same
	* Grassman's laws: Any four colors are linearly dependant, you can represent any color with 3 independent fixed colors
	* Lightness of a vector of color is described by its magnitude
	* Chromaticity of a vector of color is described by its direction
	* Colors on the spectrum are calculated using barycentric coordinates from the three primary vertices
	* Subtractive color mixing can go outside its triangle
	* To calculate the color value we use $F = rR + gG + bB$  such that a b c are color matching functions
	* Metamers are colors that have different spectral distributions but appear the same to the human eye
	* Primaries are usually RGB but we can perform basis change from one RGB to another system of primaries if we are given those primaries $$\begin{bmatrix}r(x) & g(x) & b(x)\\ r(y) & g(y) & b(y)\\ r(z) & g(z) & b(z)\\\end{bmatrix}$$
	* No Primaries inside the color curve can represent all colors by additive mixing
* #### Color Spaces:
	* XYZ color system, chosen so it tightly encompasses the curve
		* Color spaces can be used for other types of devices or more aligned with human perception
	* RGB color system
		* represented in Unit cube
		* used for pc screens
		* diagonal are gray values
	* CMYK (Cyan Magenta Yellow)
		* similar to Tinted glass
		* subtractive system, generated by removing colors from white
		* Sum to black and background is white
		* Compliment to RGB, used in printers
		* black is sometimes added to make sure the black is dark enough
	* YCrCb:
		* Lightness and chromaticity
		* Used to be used for TV but its now used for video compression
		* $Y = 0.3R + 0.59G + 0.11B$
		* Cr/I is the difference from Gray to red (R-Y)
		* Cb/Q is the difference from Blue to red (B-Y)
	* HLS:
		* Hue, Lightness, Saturation
		* Easier to use by humans
		* Projection of UNit cube along the gray diagonal, is now a wheel, and hue is given by wheel
		* Hue (Color) slice of color
		* Saturation (Intensity and vibrancy) is the distance to the center, the closer you are the more achromatic you are
		* Lightness is how white/black it is (0-1 from black to white)
		* Value is high light/dark it is (0-1 from black to color)
***
# [[Texturing]]
* #### Mapping world space to UV coords:
	* Sphere mapping : polar coordinates theta and phi that describe any point in 3d unit sphere, we can encode u and v as the angles
	* Cylinder mapping: We assume that the object z axis aligns with the height axis of the cylinder. We now describe uv as the height and angle (z=h, (x,y)=h)
	* Box mapping: We assume that we use an axis aligned bounding box, we omit the smallest axis and use the other two as uv
	* General mapping: we explicitly map uv coordinates per vertex
	* Volume mapping: mapping things in 3d, more storage and less computation (looks like object is carved out!)
* #### Magnification Texturing issues: evalutating color between different textels to color in pixels
	* Nearest Neighbour:
		* Color in the entire pixel with the rounded down integer values of the uv
	* Bilinear Interpolation: Interpolation in each direction
		* We interpolate a middleground value between top corners and bottom corners and get 2 points: 
			* v ratio = Coordinates at right- coordinates at left
			* u ratio = Coordinate at top - coordinate at bottom
			* left color = v ratio * Color(top left) + (1-vratio) * Color(bottom left)
			*  right color = v ratio * Color(top right) + (1-vratio) * Color(bottom right)
			* color = uratio * Color( right color) + (1- uratio) * Color(left color)
* #### Minification: Undersampling colors in pixels leads to aliasing (Tiling)
	* MipMapping: Create hierarchy of textures stored in the nonbottom right of a square, the next mipmap is twice as small
		* projection in uv coordinates:
			* $a=\sqrt{ \left( \frac{\partial u}{\partial x} \right)^{2} +\left( \frac{\partial v}{\partial x} \right)^2}$
			* $b=\sqrt{ \left( \frac{\partial u}{\partial y} \right)^{2} +\left( \frac{\partial v}{\partial y} \right)^2}$
			* The height of the mipmapping is decided by the log2 of the max edge (a nd b can be interpolated to be integers)
		* SAT Mipmapping doesnt account for anisotropy:
			* Dynamic programming technique to approximate axis parallel  rectangle
			* can be precompited also via dynamic programming
* #### Rasterization:
	* Rasterization of textures over primitives requires interpolation of uv, rasterization is however done after perspective transformation (vertex shader)
	* Uniform steps in image space are not the same as uniform steps in 3D especially when we have a perspective transformation
	* we can use more primitives but it results in T junctions
	* Step in image space $\frac{y_{1}}{x_{1}}+ t( \frac{y_{2}}{x_{2}} - \frac{y_{1}}{x_{1}})$
	* Step in 3d $\frac{y_{1} + s(y_{2}-y_{1})}{x_{1} + s(x_{2} - x_{1})}$
	* for the interpolated values u  $u = u_{1} + t(u_{2}-u_{1})$ and $t=\frac{sw_{2}}{w_{1} + s(w_{2}-w_{1})}$
* #### Mappings:
	*  Light mapping: offline computation of light combined in real time for static scenes
	* Environment mapping: reflecting object is small relative to the environment and it reflects a view dependant environment
	* Matcaps: used for sculpting and has color and material, determines color of pixel from view direction and direction of normal
	* displacement mapping: give normals different heights and make new geometry out of it
	* Bump mapping: only compute surface normals without modifying the geomtry and compute accordingly
	* Normal mapping: combine a texture with a normal map to compute a combined image
		* Tanget and bitanget and normal (x,y,z) defined per vertex
***
# [[RayTracing]]
* #### Recursive Raytracing:
	* Assume light is point light
	* Materials have specular and diffuse
	* Reflection and refraction are restricted to ideal and the only ones attenuated 
	* shadows come from opaque objects
	* Tracing rays from light to the eye is bad because only a small precentage of rays will hit the eye, we instead model the other way around
* #### Speeding up raytracing:
	* Adaptive recursion depth: define a max depth that the ray can go to before it stops to limit the calculations (this fraction of contribution will tend to 0)
	* Adaptive anti aliasing.
	* Spatial bounding volume hierarchy to detect intersections faster:
		* Must be simple bounding volumes to speed up detecting whether we intersect or not
		* if bounding volume is hit traverse tree, otherwise return.
		* Bresenhams algorithm in 3d to traverse a ray through a grad of bounding volumes or KD tree
	* Directional precomputation using a light buffer:
		* Precompute useful information about our light source in order to reduce light computation
		* Shoot ray out of every pixel in a grid around the light source and pre-compute the minimal distance to that light source
		* if anything reflects off of an object to that pixel and has larger distance then its visible
		* make sure to store ID of polygon so it doesnt shadow itself
* #### Raytracing phenomena:
	* Phenomena handled this way usually have Stochastic distribution of ray directions followed by average of colors
	* Glossyness/ blurr (Reflected/refracted rays are distributed according to a function)
	* Softshadows (partially occluded light shadow rays)
	* depth of field (distribution of light that all pass through the same focal point)
	* motion Blue (rays distributed over time)
* #### Advantages and disadvantages:
	* complexity of scene doesnt affect the algorithm implementation, only requires ray intersection and normal computation
	* Objects may intersect each other
	* shading is only computed for points that are relevant
	* models occlusion, shadows, reflections, perspective transformations, clipping and translucency automatically
	* need to supersample to remove aliasing (expensive)
	* slow
	* soft shadows are expensive
	* recomputed for every viewpoint, no cohernecy exploits
	* global diffuse light propagation is ignored
***
# Radiosity
* #### Rendering equation:
	* $L(x \rightarrow x') = E(x \rightarrow x') + \int_{M}\rho(x'' \rightarrow x \rightarrow x') L (x'' \rightarrow x)G(x'' \leftrightarrow x)V(x'' \rightarrow x)dA(x'')$
	* x' is camera, x is reflection surface x'' is object
	* L is Radiance coming from a specific point to the camera
	* E is the light emitted from the surface point
	* integral is over surfaces in the scene
	* Rho is the scale of reflectance of the material
	* L is the radiance coming from the surface to out reflection point
	* G is the fraction that is going to our reflection point due to angles of a surface
	* V is the visibility of the surface (0/1)
* #### Radiosity Simplification:
	* We make the following assumptions:
		* All surfaces are perfectly diffuse
		* Surfaces have the same material on every point
		* Radiosity of each surface is constant
		* G and V are combined as a form factor F
		* $B_{x} = E_{x}+ \rho_{x''} \int_{M} B_{x''}F(x'' \rightarrow x)$
		*  Discretize the scene into small surface elements
		* $B_{x}= E_{x} + p_{x}\sum_{j=1}^{N}F_{ij} B_{j}$
		* Computed a matrix for each Bi and Bj, independent of viewing direction
		* Radiosity of each element is updated by the product of the corresponding row and the current solution vector
		* Radiosity averaged over vetices to allow smoothness, and the boundaries are extrapolated
	* Form Factors: 
		* Calculated by an integral over the halfsphere of a surface light distribution divided by its volume
		* takes up most pf the computation so requires smplification
		* Hemicube algorithm:
			* Surround object by a hemicube divided onto pixels
			* project the light from a surface onto those pixels
			* sum them up and thats the form factor
			* occlusion is already included in this method, using zbuffers
		* Raycasting:
			* Generate n different random rays from one surface to another and integrate over them
		* More improvement:
			* Progressive refinement, shooting instead of collecting, incorporate bright first then dark, only one matrix row needed in each step
			* change of basis
			* cluster elements in large batches and if the solution is good enough no need to traverse down
	* Properties:
		* Independent of viewing parameters
		* Based on physical parameters so more accurate
		* can be precomputed
		* expensive
		* restricted to diffuse so cannot do reflection but can correctly color bleed and soft shadow
* #### Path-Tracing:
	* Turn radiosity equation from points in space to angles on the unit hemisphere:
	* $L(x,w_{0}) = E(x,w_{0}) + \int_{s^{2}} \rho(x,w_{0},w_{i})L(x,w_{i})|cos\theta_{i}|dw_{i}$
	* The cosine describes the geometry term such that the visibility is implicit
	* rho is the BRDF
	* infinite dimensional Integral!!
	* Monte carlo:
		* Instead of computing the integral we numerically approximate it given t iterations
		* Randomly select N positions xi uniformly distributed and compute
		* $F_{n}=\sum_{i}^{N}\frac{b-a}{N}f(x_{i})$
		* Sample defines a path through the scene in infinite dimensional but is limited to D
		* can be iteratively built by tracing rays from camera
	* Adjustments:
		* Some areas contribute a lot more than other areas, we replace the uniform distribution with something that puts more weight on incoming light
		* randomly reaching the light is unlikely, thus we explicity send rays towards the light because we are gonna compute it anyway, we calculate radiance at each point along the path
		* Denoising
***
# [[Animation]]
* #### Keyframe animation
	* We define positions at different key frames in time and interpolate between them or by specifying the transformations that occur
	* Rigid body transformations (Whole body moves together): we specify the local to global matrix in the scene graph over time
		* How do we interpolate?
			* The sum of two rotation matrices element-wise is typically not a rotation matrix $(1-t)R_{0} +tR_{1} = R_{t}$ so cannot use Linear interpolation
			* Interpolation using Euler angles stays as a rotation but will not have a logical path to distination
			* Interpolating using log to get to tanget space then performing linear interpolation then going back with the exponential is the correct way
	* Tweening adds "ease in, ease out" mechanic and sometimes adds unintentional bouncing effects using spline curves
* #### Linear Blend Skinning
	* Idea: animate each vertex's transformation on its own to allow for bodypart movement
	* problem: unwanted shrinking and growing effects
	* solution rigging: Skeletal hierarchy of bones, each bone affects vertices with different weights, we move the bones and transform the vertices accordingly:
		* Weights are non-negative and sum to 1 which makes the object invariant to rigid transformations
		* Compute restpose local to global transformations which describe how to go from bone position to global $\hat{G}_{j}= \hat{G}_{j-1}L_{j} = L_{1}L_{2}L_{3}....$ 
		* To move a vertex, we send its global coordinates to bone restpose and then perform the global transformation of bone j on it $p_{ij} = G_{j}G_{j}^{'-1} p_{i}$
		* This has to be done for every bone thats affecting pi and weighted summed, for every vertex pi and normal ni
* #### Dual quaternions
	* Recap:
		* We represent Rotation in 3d using 3 parameters and encoding the fourth $v = \text{ Rotation Axis such that } \|v\|=\theta$
		* We represent orientation in 3d using 4 parameters: $e_{0} = \cos\frac{\theta}{2}$ and $\begin{bmatrix}e_{1} \\ e_{2} \\ e_{3}\end{bmatrix} = v\sin\frac{\theta}{2}$ and v is a unit axis vector
		* Canonical orientation is represented via $\begin{bmatrix} 1 \\ 0 \\ 0\\ 0\end{bmatrix}$ which we can encode with unit quaternions
		* Quaternions are $q=w +ix + jy+ kz$ such that $w^2 + x^2 + y^2 + k^2 = 1$ and $i^2 = j^2 = k^2 = ijk = -1$ and $ij = k , ji=-k$ 
		* Quaternion Multiplication $q_{1}q_{2}=(w_{1}w_{2} - v_{1}\dot{v_{2}} , w_{1}v_{2} + w_{2}v_{1} + v_{1}\times v_{2})$
		* SLERP $q_{1} = e^{t\log(q^-1q_{2})}$
		* Rotation around an arbitrary axis $p'=qpq^{-1}$ such that p is a point we want to rotate around v with angle $|\theta|$ and q is a quaternion representing an orientation
	* We Used quaternions to represent rotations, we now use Dual quaternions to represent rigid transformations (translation and rotation)
	* Dual Numbers:
		* $a = a_{0}+ \epsilon a_{\epsilon}$ such that $\epsilon^{2}=0$
		* $\epsilon = \begin{bmatrix} 0 & 1 \\ 0 & 0\end{bmatrix}$ which means $a = \begin{bmatrix} a_0 & a_{\epsilon} \\ 0 & a_{0}\end{bmatrix}$
		* Addition is performed by adding the part
		* Multiplication works by multiplying the real numbers and cross product the imaginary
	* Dual quaternions are comprised of 2 quaternions linked dualy $\hat{q} = q_{0} + \epsilon q_{\epsilon}$
	* A dual quaternion represents a point by setting $q_{0}$ to no orientation (1,0,0,0) and $q_{\epsilon}$ to an imaginary quaternion encoding the point
	* a dual quaternion represent an orientation setting $q_{0}$ to an orientation and $q_{\epsilon}$ to 0
	* Rotation of a point around an arbitrary axis $\hat{p} = \hat{r}\hat{p}\hat{r}^{**}$ which will keep $q_{0}$ at identity orientation in the dual and actually transform point coordinates in the dual imaginary part to the respective position
	* Translation of a point is also possible through dual quaternions by constructing a psudeo point divided by 2, and performing the same operation as rotation
	* thus dual quaternions allow us to use quaternions to rotate AND translate in the same operation while still being a unit quaternion
	* now we Blend between 2 unit quaternions and then normalize to get back a unit quaternion (not uniform speed!)
* #### Blendshapes
	* Facial animations are a weighted sum of 48 different facial expressions (assuming all blendshapes have same triangulation and vertices)
	* Or by performing expression displacement such that we get the distance between each expression and the neutral face
***
# [[SImulation]]
* #### Differential equations:
	* relates an unknown function to its derivative in order to be able to simulate function of states over time
	* We start off by defining the initial state at t=0
	* we compute the following states numerically, at least one state per frame
	* Newtonian mechanics:
		* $F=ma$ and $F=m\frac{dv}{dt}$ and $F=m\frac{d^{2}p}{d^{2}t}$
		* We are interested in knowing the follow states of the position and the following states of the velocity
		* $\frac{d}{dt}p = v$ and $\frac{d}{dt}v = F/m$
	 * Gravity = $g =(0,0,-9.807)$
	* Dampening = $F_{d}= -kv$
	* Spring force = $F_{S} = -k(\|d\| - r)\frac{d}{\|d\|}$
* #### Euler's Method:
	* replace dt with a fixed small constant h, if h is sufficiently small we approximate the function better
	* $\begin{bmatrix} p_{i+1} \\ v_{i+1} \end{bmatrix} = \begin{bmatrix} p_{i} \\ v_{i} \end{bmatrix} + h\cdot f(\begin{bmatrix} p_{i} \\ v_{i} \end{bmatrix})$
	* $f(\begin{bmatrix} p_{i} \\ v_{i} \end{bmatrix}) = \begin{bmatrix} v_{i} \\ \frac{F}{m} \end{bmatrix}$
	*  Euler's method suffer from inaccuracies , error decreases linearly O(h)
* #### Trapezoid Method: 
	* If your arrival point is inaccurate, compensate for the error by averaging over the next arrival point
	* $f_{i} = f(x,t_{i})$
	* $f_{i+1}=f(x_{i}+h\cdot f_{i},t_{i+1})$
	* $x_{i} = x_{0} +\frac{h}{2}(f_{i} + f_{i+1})$
	* Error decreases quadratically $O(h^2)$
	* There are other methods that decrease the error quadratically $O(h^4)$ $x_{i+1} = x_{i} + \frac{6}{h} \left(f_{0}(x_{0}) + 2 \cdot f_{1}(x_{0} + \frac{h}{2} f_{0})+ 2 \cdot f_{2}\left(x_{0} + \frac{h}{2}f_{1}\right)+ f_{3}(x_{0}+ hf_{2}) \right)$
* #### Implicit Euler:
	* While explicit Euler overtime exaggerates the motion, Implicit Euler overtime dampens the motion! which is a lot more acceptable
	* Implicit euler still has the same errors of Explicit euler, its just never gonna explode
	* Instead of moving to the next point via the gradient at the current point, we see which next opposite gradient leads to the current one
	* $x_{i+1} = x_{i} + hf(x_{i+1})$
	* We approximate this linear system of equations via Newtons method:
		* to find g(x) = 0 we first guess a random $x_0$
		* we set $g(x_{i+1}) = x_{i+1} - x_{i} - hf(x_{i+1}) = 0$ which results in the Jacobian being $I -hJ_{f}(x_{i})$
		* Compute $g'(x_{i})\Delta_{x}= -g(x_{i})$ or $J_{g}\Delta x = -g(x_{i})$ (Usually we need to iterate over it but one step is enough for approximations)
		* We update the state $x_{i+1}=x_{i}+ \Delta_{x}$
* #### Rigid Body Simulations:
	* Our rigid body is defined as a center of mass + a difference vector $p_{i}= p + r_{i}$
	* When we rotate we rotate the vector $r_{i}$
	* When we translate we translate the center of mass
	* Linear Velocity is the Velocity of the center of mass
	* Angular Velocity is the change of angles
	* in angular Dynamics F -> Torque and linear momentum -> Angular momentum
***
# [[Output mapping]]
* #### Types of images:
	* Vector Images:
		* SVG, pdf partially
		* Good for Geomtry and lines and points (2D Graphics)
	* Raster Images:
		* Png lossless, Jpeg lossy, GIF
			* Paints stuff on pixels, resolutions are number of pixels in the image which dictate quality
			* not suited for lines and text
* #### Tone-Mapping:
	* Mapping infinite tones to 256
	* if we clamp, things will be too white,
	* if we linearly interpolate things will be too dark,
	* we need to interpolate according to human vision and give less weight to brighter colors
	* Reinhard Tonemapping:
		* $L_{out} = \frac{L_{in}}{1+L_{in}}$
		* $C_{out}= C_{in}\frac{L_{out}}{L_{in}+\epsilon}$
		* Problem, no value maps to perfect white
		* burnout is the decreased contrast for high luminance
		* Clamping is still necessary
	* Extended Reinhard tonemapping:
		* $L_{out} = \frac{L_{in} + \left(1+\frac{L_{in}}{L_{white}} \right)}{1+L_{in}}$
		* $L_{white}$ is set to the lowest value that will be clamped to white
* #### Gamma Correction:
	* Encode images as Gamma corrected version $V = C^{\frac{1}{\gamma}}$
	* Discretize it, by doin so we give more details to darker spots of the image
* #### Color Tables:
	* 24 bits per color is often unecessary to describe an image, too much choices
	* we can often simplify images to 256 different colors
	* We create a table with 256 different 24bit colors and use 8bits per pixel as a reference to an entry in the color table
		* Median Cut algorithm:
			* Take bounding rectangle in color space:
			* split on the median
			* take the next longest diagonal rectangle
			* split on the median and repeat until 256 boxes
			* pick color of box as the center of the rectangle
* #### Dithering:
	* If our output contains less color options we often have to dither the image
	* its a type of noise to reduce quantization error
	* Clamping is not good!
	* Floyd-Steinberg:
		* We iterate over pixels from top left row wise
		* We look at the real value of the current pixel we are at
		* We make a decision based on our current error
			*  If our error is 0 we set it the pixel to 0
			* if our error is too bright we set the pixel to 0
			* if our error is too dark we set the pixel to 1
		* We compute the error induced by out decision
			* $Error = Error_{accumlated} + (C_{origin} - C_{decision})$
			* Positive error indicates Darkness and Negative error indicates lightness
		* We propagate the error the nearby pixels according to mask
* #### JPEG compression:
	* Convert from RGB to YCrCb space:
		* Humans are a lot more sensitive to light than color, this transformation allows us to reduce the information of chromaticity without messing with the lightness
	* Chroma subsampling:
		* Lossy operation that stores less information for CrCb part
		* Averaged over small regions described over a 2x2 square (e.g 4:2:2)
		* Vertical resolution is only reduced if Cb = 0
		* Horizontal resolution is determined by the 2nd parameter
	* DCT:
		* Another transformation that converts our image into frequency space
		* the goal is to take sufficiently small squares of pixels in the image, those squares usually wont have much color variation thus there wont be many transitions and thus the high frequencies produced by DCT are negligible, Which means we can group their high coeffecients together and keep the low coefficients in detail and we will describe the image in quite a similar manner!
		* Humans are less sensitive to high frequencies
		* This is lossless but it prepared for a lossy operation
		* maps colors from -128 to 127
	* Quantization:
		* Lossy operation that Quantizes the frequency coefficients, focusing more detail on the lower ones and less details (bigger quantization groups) to high frequency coefficients, which reduces information content and less storage space to store the description of the image
		* Usually reduces a lot more information for Chrominance than luminosity 
		* Creates Quantization tables that go from low spatial frequency to high spatial frequency, described vertically and horizontally
	* Entropy Encoding:
		* Serialize data from low frequency to high frequency in a zigzag manner from top left
		* Transformed into a bit stream
		* Encoded differentially (Difference to the previous coefficient) for smaller space, usually long lists of zeros
		* Result bitstream is encoded using Entropy encoding